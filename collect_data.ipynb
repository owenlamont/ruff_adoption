{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from contextlib import suppress\n",
    "from datetime import datetime, UTC\n",
    "import math\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "from github import Auth, Github\n",
    "from github.GithubException import UnknownObjectException\n",
    "from requests.exceptions import RetryError\n",
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2",
   "metadata": {},
   "source": [
    "## Github Token\n",
    "\n",
    "This notebook expects a Github token either as an environment variable named GITHUB_API_TOKEN or in a .env file in the root directory of this repo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "auth = Auth.Token(os.getenv(\"GITHUB_API_TOKEN\"))\n",
    "g = Github(auth=auth)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4",
   "metadata": {},
   "source": [
    "## Description\n",
    "\n",
    "The following logic searches Python repos in descending order of stars and checks filters out repos younger than _min_created_at_ date or if it fails to find a .pre-commit-config.yaml file.\n",
    "\n",
    "## Warnings\n",
    "\n",
    "1. Due to the Github REST API rate limits the following cell takes a long time to run, with the default arguments this took 8+ hours to scrape the .pre-commit-config.yaml files for 1000 repos.\n",
    "2. The Github REST API has a nasty gotcha where if you try to search too many repos at once (i.e. have too broader filter criteria) it will time out and return some results - but those results will be time-sensitive and non-deterministic (i.e. when I tried searching for all repos at once in descending order of stars it would hit this timeout issue and randomly omit different repos that should've been included in the results). That was the reason for adding the bounds_scaling variable below where it only searches for repos between certain min and max star count values and those bounds reduce each iteration. bounds_scaling should be less than 1 and greater than zero but probably needs to remain quite close to 1, i.e. 0.85, 0.9, or 0.95 to prevent the timeout issues that cause non-deterministic reuslts.\n",
    "\n",
    "## Potential Improvements\n",
    "\n",
    "There's a number of potential improvements that could be made to the efficiency and coverage of this data collection, namely:\n",
    "\n",
    "1. Further tweak the Github repo query strings to exclude archived repos or filter out repos that have no changes after the min_created_at date to filter out useless repos earlier in the pipeline.\n",
    "2. This script aggregates commits to just take the most recent commit per month. You could add logic to inspect what commits actually touched the .pre-commit-config.yaml though and specially keep all of them to get the highest time fidelity change data.\n",
    "3. There's no way a present (that I'm aware of) with the Github REST API search function to filter repos by the presence of a file (i.e. the .pre-commit-config.yaml) so this logic spends a lot of time manually checking the contents of repos in the search results. If it becomes possible in the future to just search repos on the presence of a particular file this code would probably run much faster (or maybe the filename could be searched first and then repos that contain it iterated instead)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_repos_names = set()\n",
    "precommit_repo_list = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "# If this cell fails re-run and it should resume (it won't double count recorded repos)\n",
    "%%time\n",
    "pre_commits_path = Path(\"pre-commits\")\n",
    "min_created_at = datetime(year=2022, month=6, day=1, tzinfo=UTC)\n",
    "saved_repo_count = sum(1 for subdirectory in pre_commits_path.iterdir() if subdirectory.is_dir()) if pre_commits_path.exists() else 0\n",
    "required_repos = 1_000 - saved_repo_count\n",
    "max_stars = 100000\n",
    "bounds_scaling = 0.95\n",
    "query = f\"language:python pushed:>{min_created_at:%Y-%m-%d} stars:>{math.floor(max_stars * bounds_scaling)} archived:false\"\n",
    "with tqdm(total=required_repos) as pbar:\n",
    "    while len(precommit_repo_list) < required_repos:\n",
    "        result = g.search_repositories(query, sort=\"stars\", order=\"desc\")\n",
    "        repo_list = list(result)\n",
    "\n",
    "        print(f\"{query} results={len(repo_list)}\")\n",
    "\n",
    "        # This checked_repo_details logic was just for debugging the repo search was progressing as expected\n",
    "        checked_repos_log = Path(\"checked_repo_details.txt\")\n",
    "        if checked_repos_log.is_file():\n",
    "            with checked_repos_log.open(\"a\") as file:\n",
    "                for repo in repo_list:\n",
    "                    file.write(f\"{repo.full_name} {repo.stargazers_count} {repo.created_at}\\n\")\n",
    "        else:\n",
    "            with checked_repos_log.open(\"w\") as file:\n",
    "                for repo in repo_list:\n",
    "                    file.write(f\"{repo.full_name} {repo.stargazers_count} {repo.created_at}\\n\")\n",
    "\n",
    "        for repo in repo_list:\n",
    "            with suppress(UnknownObjectException, AssertionError, RetryError):\n",
    "                pre_commit_config = repo.get_contents(path=\".pre-commit-config.yaml\")\n",
    "                if repo.full_name in processed_repos_names:\n",
    "                    continue\n",
    "                successful_save = False\n",
    "                branch = repo.get_branch(repo.default_branch)\n",
    "                commits = repo.get_commits(sha=branch.commit.sha, since=min_created_at)\n",
    "                commit_list = list(commits)\n",
    "\n",
    "                monthly_commits = {}\n",
    "                for commit in commit_list:\n",
    "                    commit_date = commit.commit.author.date\n",
    "                    key = commit_date.strftime(\"%Y-%m\")\n",
    "                    if key not in monthly_commits or commit_date > monthly_commits[key].commit.author.date:\n",
    "                        monthly_commits[key] = commit\n",
    "\n",
    "                monthly_commit_list = sorted(monthly_commits.values(), key=lambda commit: commit.commit.author.date)\n",
    "                monthly_commit_list = [commit for commit in monthly_commit_list if commit.commit.author.date >= min_created_at]\n",
    "\n",
    "                for commit in monthly_commit_list:\n",
    "                    with suppress(UnknownObjectException, AssertionError):\n",
    "                        pre_commit_config = repo.get_contents(\".pre-commit-config.yaml\", ref=commit.sha)\n",
    "                        save_path = Path(\n",
    "                            f\"\"\"pre-commits/{repo.full_name.replace(\"/\", \"_\")}/\"\"\"\n",
    "                            f\"\"\"{commit.commit.author.date:%Y-%m}/.pre-commit-config.yaml\"\"\"\n",
    "                        )\n",
    "                        save_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "                        save_path.write_bytes(pre_commit_config.decoded_content)\n",
    "                        successful_save = True\n",
    "\n",
    "                if successful_save:\n",
    "                    precommit_repo_list.append(repo)\n",
    "                    processed_repos_names.add(repo.full_name)\n",
    "                    pbar.update(1)\n",
    "\n",
    "                if len(precommit_repo_list) >= required_repos:\n",
    "                    break\n",
    "        max_stars = math.floor(max_stars * bounds_scaling)\n",
    "        if len(repo_list) > 0:\n",
    "            max_stars = min(repo_list[-1].stargazers_count, max_stars)\n",
    "        query = (\n",
    "            f\"language:python pushed:>{min_created_at:%Y-%m-%d} stars:{math.floor(max_stars * bounds_scaling)}..{max_stars} archived:false\"\n",
    "        )\n",
    "\n",
    "# Save ordered list of repos used (just used for debugging)\n",
    "with open(\"repo_names.txt\", \"w\") as file:\n",
    "    for repo in precommit_repo_list:\n",
    "        file.write(repo.full_name + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(processed_repos_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
